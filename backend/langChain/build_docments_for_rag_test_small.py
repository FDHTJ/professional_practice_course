import os
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# ================= é…ç½®åŒºåŸŸ =================
# 1. è®¾ç½®ä½ ä¸‹è½½çš„ source æ–‡ä»¶å¤¹è·¯å¾„ (è¯·ä¿®æ”¹è¿™é‡Œï¼)
DOCS_PATH = "./documents"

# 2. æŒ‡å®šæˆ‘ä»¬è¦ä¿å­˜å‘é‡æ•°æ®åº“çš„è·¯å¾„
DB_SAVE_PATH = "./database_faiss_pytorch_base"

# 3. é€‰å®š Embedding æ¨¡å‹ (å…³é”®ï¼è¿™é‡Œé€‰ BGE-M3 æ”¯æŒä¸­è‹±äº’æœ)
# ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ (çº¦ 500MB+)ï¼Œè¯·ä¿æŒç½‘ç»œé€šç•…
EMBEDDING_MODEL_NAME = "BAAI/bge-base-zh-v1.5"


# ===========================================

def load_documents(base_path):
    """
    åªåŠ è½½é«˜è´¨é‡çš„æ–‡ä»¶å¤¹ï¼šuser_guide å’Œ notes
    """
    documents = []
    # æˆ‘ä»¬åªå…³å¿ƒè¿™ä¸¤ä¸ªå«é‡‘é‡æœ€é«˜çš„æ–‡ä»¶å¤¹
    target_folders = ["user_guide", "notes"]

    print(f"ğŸ” å¼€å§‹æ‰«æè·¯å¾„: {base_path}")

    for folder in target_folders:
        folder_path = os.path.join(base_path, folder)
        if not os.path.exists(folder_path):
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹ {folder_path}ï¼Œè·³è¿‡ã€‚")
            continue

        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ–‡ä»¶å¤¹: {folder} ...")

        # åŠ è½½ Markdown æ–‡ä»¶
        loader_md = DirectoryLoader(folder_path, glob="**/*.md", loader_cls=TextLoader, show_progress=True)
        docs_md = loader_md.load()

        # åŠ è½½ RST æ–‡ä»¶ (ç®€å•ä½œä¸ºæ–‡æœ¬åŠ è½½)
        loader_rst = DirectoryLoader(folder_path, glob="**/*.rst", loader_cls=TextLoader, show_progress=True)
        docs_rst = loader_rst.load()

        # ç»™æ–‡æ¡£æ‰“æ ‡ç­¾ï¼Œæ–¹ä¾¿ä»¥åè¿½è¸ªæ¥æº
        for doc in docs_md + docs_rst:
            doc.metadata["source_type"] = "guide" if folder == "user_guide" else "technical_note"

        documents.extend(docs_md + docs_rst)
        print(f"   -> æ–‡ä»¶å¤¹ {folder} åŠ è½½äº† {len(docs_md) + len(docs_rst)} ä¸ªæ–‡ä»¶")

    print(f"ğŸ‰ æ‰€æœ‰æ–‡æ¡£åŠ è½½å®Œæ¯•ï¼Œå…± {len(documents)} ä¸ªæ–‡ä»¶ã€‚")
    return documents


def split_documents(documents):
    """
    æŠŠé•¿æ–‡æ¡£åˆ‡æˆå°å—ï¼Œæ–¹ä¾¿æ£€ç´¢
    """
    print("âœ‚ï¸ æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # æ¯ä¸ªå—çš„å¤§å°
        chunk_overlap=200,  # é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢åˆ‡æ–­ä¸Šä¸‹æ–‡
        separators=["\n\n", "\n", " ", ""]  # ä¼˜å…ˆæŒ‰æ®µè½åˆ‡åˆ†
    )
    splits = text_splitter.split_documents(documents)
    print(f"   -> åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(splits)} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚")
    return splits


def build_vector_db(splits):
    """
    å‘é‡åŒ–å¹¶ä¿å­˜
    """
    print(f"ğŸ§  æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ ({EMBEDDING_MODEL_NAME})... (åˆæ¬¡è¿è¡Œéœ€ä¸‹è½½)")
    # ä½¿ç”¨ BGE-M3ï¼Œè¿™æ˜¯ç›®å‰æœ€å¼ºçš„å¼€æºå¤šè¯­è¨€æ¨¡å‹ä¹‹ä¸€
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

    print("ğŸš€ æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“ (è¿™ä¸€æ­¥å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    db = FAISS.from_documents(splits, embeddings)

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°æœ¬åœ°: {DB_SAVE_PATH}")
    db.save_local(DB_SAVE_PATH)
    print("âœ… çŸ¥è¯†åº“æ„å»ºæˆåŠŸï¼")


if __name__ == "__main__":
    # 1. åŠ è½½
    raw_docs = load_documents(DOCS_PATH)

    if raw_docs:
        # 2. åˆ‡åˆ†
        chunks = split_documents(raw_docs)

        # 3. å»ºåº“
        build_vector_db(chunks)
    else:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")