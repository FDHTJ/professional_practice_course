import os

from langchain_classic.agents import initialize_agent, AgentType
from langchain_classic.memory import ConversationBufferMemory
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.tools import Tool
from langchain_huggingface import HuggingFaceEmbeddings

class StreamPrintHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        print(token, end="", flush=True)
# ================= é…ç½®åŒºåŸŸ =================
# æ›¿æ¢ä½ çš„ Key
os.environ["OPENAI_API_KEY"] = "495336b5c3a44a85b7b97b64da809573.Hbr3XNqsR3KKNxWO"
os.environ["OPENAI_API_BASE"] = "https://open.bigmodel.cn/api/paas/v4/"

DB_PATH = "./database_faiss_pytorch_base"
EMBEDDING_MODEL = "BAAI/bge-base-zh-v1.5"
MODEL_CACHE = "./models"


# ===========================================

def build_agent():
    print("ğŸ› ï¸ æ­£åœ¨ç»„è£… PyTorch æ™ºèƒ½ä½“...")

    # 1. å‡†å¤‡ RAG å·¥å…· (ä½ çš„çŸ¥è¯†åº“)
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True},
        cache_folder=MODEL_CACHE
    )
    db = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
    def search_pytorch_docs(query: str) -> str:
        docs_with_scores = db.similarity_search_with_score(query, k=5)

        results = []
        for doc, score in docs_with_scores:
            source = doc.metadata.get("source_type", "unknown")
            entry = (
                f"ğŸ“„ æ¥æº: {source}\n"
                f"ğŸ” ç›¸ä¼¼åº¦: {score:.4f}\n"
                f"------ å†…å®¹ ------\n"
                f"{doc.page_content}"
            )
            results.append(entry)

        return "\n\n==========================\n\n".join(results)
        # docs = db.similarity_search(query, k=5)
        # return "\n\n".join([d.page_content for d in docs])

    import torch


    def safe_python_exec(code: str) -> str:
        """æ‰§è¡Œ Python ä»£ç å¹¶è¿”å›è¾“å‡ºï¼Œä¸“é—¨ç”¨äº shape æ£€æŸ¥ã€‚"""
        try:
            # é™åˆ¶å¯ç”¨å˜é‡ï¼Œé¿å…å±é™©æ“ä½œ
            local_env = {"torch": torch}
            code=code.replace("Observation","")
            exec(code, {}, local_env)
            return str(local_env.get("out", "æ‰§è¡ŒæˆåŠŸ"))
        except Exception as e:
            return f"Python é”™è¯¯: {e}"

    # Python ä»£ç æ‰§è¡Œå·¥å…·
    python_tool = Tool(
        func=safe_python_exec,
        description="å½“ä½ éœ€è¦è¿è¡Œ Python ä»£ç éªŒè¯å¼ é‡ shape æˆ–è¿è¡Œ PyTorch ä»£ç æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥å¿…é¡»æ˜¯çº¯ Python/PyTorch ä»£ç ã€‚",
        name="python_exec"
    )

    rag_tool = Tool(
        func=search_pytorch_docs,
        name="search_pytorch_docs",
        description="é‡åˆ°PyTorchæ¦‚å¿µã€API ç”¨æ³•æˆ–æŠ¥é”™ä¿¡æ¯ æˆ–è€…å…¶ä»–pytorchç›¸å…³çš„çŸ¥è¯†æ—¶ï¼Œå¿…é¡»å…ˆç”¨è¿™ä¸ªå·¥å…·æŸ¥é˜…å®˜æ–¹æ–‡æ¡£ã€‚"
    )
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True  # è®© Agent çœ‹åˆ°å¤šè½®å¯¹è¯
    )
    # 4. æ„é€  Agent
    llm = ChatOpenAI(model="glm-4.5-flash",
                     temperature=0,streaming=True,
                     callbacks=[StreamPrintHandler()])

    def make_learning_plans(information:str):
        prompt=f'''ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ è®¡åˆ’åˆ¶å®šä¸“å®¶,è¯·æ ¹æ®ä¸‹é¢æ‰€ç»™å‡ºçš„ä¿¡æ¯,ä¸€æ­¥ä¸€æ­¥ä¸ºç”¨æˆ·åˆ¶å®šå‡ºåˆ‡å®å¯è¡Œçš„å­¦ä¹ è®¡åˆ’:
                    ä¿¡æ¯:{information}
                æŒ‡ä»¤ï¼šè¯·é‡ç‚¹å…³æ³¨ç”¨æˆ·çš„å­¦ä¹ ç›®æ ‡ã€æ—¶é—´æœŸé™ç­‰ä¿¡æ¯,è€Œä¸”è¦å°½å¯èƒ½çš„é€šè¿‡å¯¹è¯å†å²æˆ–è€…è¯¢é—®ä»¥äº†è§£ç”¨æˆ·æ‰€å¤„æ°´å¹³,ä¸ºç”¨æˆ·é‡èº«å®šåˆ¶å‡ºåˆé€‚çš„è®¡åˆ’.
                å¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›è§„åˆ’åçš„ç»“æœï¼š
                1.æ€»ä½“ç›®æ ‡(æ€»ä½“éœ€è¦è¾¾åˆ°çš„æ°´å¹³ä»¥åŠæ‰€ç”¨æ—¶é—´åŠç”¨æˆ·æ‰€å¤„æ°´å¹³)
                2.åˆ†é˜¶æ®µè®¡åˆ’
                    -é˜¶æ®µä¸€(é˜¶æ®µé¢„è®¡ç”¨æ—¶):[é˜¶æ®µä¸»é¢˜]
                        *å­¦ä¹ å†…å®¹:[å…·ä½“å­¦ä¹ å†…å®¹åˆ—è¡¨]
                        *æ¨èèµ„æº:[ä¹¦ç±ã€ç½‘ç«™ã€è¯¾ç¨‹ç­‰]
                        *è¯„ä¼°æ–¹å¼:[æµ‹è¯•ã€ç»ƒä¹ ã€é¡¹ç›®ç­‰]
                    -é˜¶æ®µäºŒ...
                3.è¯„ä¼°å’Œè°ƒæ•´:[å¦‚ä½•è¯„ä¼°è¿›åº¦å’Œè°ƒæ•´è®¡åˆ’]
                '''
        return llm.invoke(prompt).content
    planning_tool=Tool(
        func=make_learning_plans,
        name="make_learning_plans",
        description="å½“éœ€è¦ä¸ºç”¨æˆ·åˆ¶å®šå­¦ä¹ è®¡åˆ’æ—¶è°ƒç”¨æ­¤å·¥å…·å¸®åŠ©ä½ ä¸ºå…¶åˆ¶å®šæ›´å®Œå–„åˆç†çš„å­¦ä¹ è®¡åˆ’",
    return_direct = True
    )

    agent = initialize_agent(
        tools=[rag_tool,python_tool,planning_tool],
        llm=llm,
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True,
        memory=memory
    )



    return agent


if __name__ == "__main__":
    agent = build_agent()
    print("\nâœ… å…¨èƒ½ PyTorch åŠ©æ‰‹å·²å°±ç»ªï¼")

    while True:
        user_input = input("\nğŸ™‹ è¯·æé—® (qé€€å‡º): ")
        if user_input.lower() == 'q':
            break

        try:
            # Agent å¼€å§‹è‡ªåŠ¨è§„åˆ’å’Œæ‰§è¡Œ
            response = agent.invoke({"input": user_input})
            # print("\nğŸ¤– æœ€ç»ˆå›ç­”:", response['output'])
        except Exception as e:
            print(f"âŒ å‡ºé”™äº†: {e}")